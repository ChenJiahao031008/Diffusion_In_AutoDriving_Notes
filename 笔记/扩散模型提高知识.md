# 扩散模型提高知识

[TOC]

------------------

## Diffusion Transformer（DiT）

### 1. 关键点分析

1. **转成Transform**：同样使用 VAE, 并对不同的 patch 赋予 token （patch 2/4/8）

### 2. Norm layer 归一化知识点汇总

1. **标准公式如下**

$$
y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} + r + \beta
$$

2. **对于 BN 和 LN**
   - **BN**: 对 (B,N,D) 中 D 维归一化
   - **LN**: 对 (B,N,D) 中 B 维归一化
     - PostNorm: 性能更好，但是更难优化（残差随层数呈指数增加，容易梯度消失或爆炸）
     - PreNorm: 网络结构简单，性能较差（保留更多残差信息），因此PreNorm更优效果，更常用。

3. **对于Adapt-Zero Norm**

   + **Norm layer**：γ、β为可学习参数，在训练中学习但固定应用于所有输入  
   + **Adapt Norm layer**   
     1.  基于条件信息动态生成：$γ, β = $MLP(condition)   
     2. 通过通道注意力自适应生成：$γ, β = f(x)$  
     3. 本文中Adaptive 条件嵌入：时间步嵌入与类别嵌入被融合为条件向量

   + **Adapt-Zero Norm**： $\gamma =1,\ \beta=0$，使初始化阶段模型接近标准归一化。本文中额外学习一个逐channel的参数$\alpha$（门控参数，残差初始贡献为0） 

### 2. 三种 DiT Block 设计

1. 条件和图像直接相加
2. 使用 cross-attention
3. 加入 adapt Norm layer 层（3.1 不同网络添加位置）->  效果最好（3.2 使用全零初始化）

<img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250329220318589.png" alt="image-20250329220318589" style="zoom:80%;" />

## Low Rank Adaptation (LoRA)

https://arxiv.org/pdf/2106.09685

###  1. LoRA的出现的原因

+ 使用LORA，训练参数仅为整体参数的万分之一、GPU显存使用量减少2/3且不会引入额外的推理耗时

### 2. 低秩假设：LoRA的理论基础

**预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果**。具体而言：

1. **权重更新的低秩特性**：矩阵秩表示其线性无关的行或列的数量。研究发现，微调过程中权重的变化ΔW可以用远低于其原始维度的向量组合表示。
2. **内在维度**：在预训练后，越大的模型有越小的内在维度，这也解释了为何大模型都拥有很好的few-shot能力。

### 3. LoRA微调

![image-20250323165613779](%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250323165613779.png)

公式： $h = W_0x+ \Delta Wx = W_0x + \frac{\alpha}{r}\cdot B Ax$。

+ 其中 $W_0 \in \R ^{(d\times r)}$ 是预训练的权重矩阵，$\Delta W = BA$ 就是我们添加的额外矩阵，$B \in \R ^ {(d \times r)}, A \in \R^{r\times k}$，$r \ll \min(d,k)$ 表示矩阵的秩，为超参。

+ 初始化时，矩阵 A 随机高斯初始化$N(0, \sigma^2)$初始化，其中$\sigma = 1/\sqrt{r}$，矩阵 B 初始化为0：在初始阶段这两个矩阵相乘为0，可以保证在初始阶段时，只有左边的主干生效。然后 BA 还会乘以一个缩放因子 $\frac{\alpha}{r}$ ，α也是超参。
+ 训练的时候，预训练的权重矩阵全部都是冻结的，仅训练A和B中的参数。

### 4. Lora 实现细节

1. **参数合并机制** ：LoRA不会增加额外的推理时间成本

   + **微调后合并权重** ：在LoRA训练完成后，通常会将低秩增量矩阵（ΔW）直接合并到原始模型的权重矩阵（W）中，形成新的权重矩阵 W′=W+ΔW。**推理时直接使用合并后的权重**，无需额外计算步骤，因此计算复杂度与原始模型完全一致。

2. LoRA版本的全连接层：

   ```python
   # 在调用凯明初始化的时候注释里写的高斯分布，调用的却是均匀分布，而且参数a的值设置的是根号5，但a表示的是leaky relu的负斜率系数，一般是0.01这样的小值，不可能超过1
   def reset_parameters(self):
       nn.Linear.reset_parameters(self)
       if hasattr(self, 'lora_A'):
           # initialize A the same way as the default for nn.Linear and B to zero
           nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
           nn.init.zeros_(self.lora_B)
           
   def forward(self, x: torch.Tensor):
       def T(w):
           return w.transpose(0, 1) if self.fan_in_fan_out else w
       if self.r > 0 and not self.merged:
           result = F.linear(x, T(self.weight), bias=self.bias)            
           result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling
           return result
       else:
           return F.linear(x, T(self.weight), bias=self.bias)
   ```

3. LoRA版本的卷积层：

   将增量 $\Delta W \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}} \times K_h \times K_w}$ 分解为两个低秩矩阵：
   - $$ A \in \mathbb{R}^{C_{\text{out}} \times r \times 1 \times 1} $$（逐点卷积，调整通道维度）
   - $$ B \in \mathbb{R}^{r \times C_{\text{in}} \times K_h \times K_w} $$（低秩卷积核，捕捉空间特征）  

   ```python
    ## CASE 1: https://blog.csdn.net/weixin_54338498/article/details/136811439
       if isinstance(module, nn.Conv2d):
           assert len(self._up_weight.shape) == len(self._down_weight.shape) == 4
   
           r = self._r
           in_dim = module.in_channels
           out_dim = module.out_channels
           kernel = module.kernel_size
           stride = module.stride
           padding = module.padding
   
           self._lora_down = nn.Conv2d(in_dim, r, kernel, stride, padding, bias=False)
           self._lora_up = nn.Conv2d(r, out_dim, (1, 1), (1, 1), bias=False)
   
       self._lora_down.weight = nn.Parameter(self._down_weight)
       self._lora_up.weight = nn.Parameter(self._up_weight)
       
    ## CASE 2: https://zhuanlan.zhihu.com/p/658007966
    ##         https://github.com/microsoft/LoRA/blob/main/loralib/layers.py
   	def __init__(....):
           self.lora_A = nn.Parameter(
               self.conv.weight.new_zeros((r * kernel_size, in_channels * kernel_size))
           )
           self.lora_B = nn.Parameter(
             self.conv.weight.new_zeros((
                 out_channels//self.conv.groups * kernel_size, r * kernel_size))
           )
           self.scaling = self.lora_alpha / self.r
       
       def train(self, mode=True):
   		...
           self.conv.weight.data += 
               (self.lora_B @ self.lora_A).view(self.conv.weight.shape) * self.scaling
       	self.merged = True
   
       def forward(self, x):
           if self.r > 0 and not self.merged:
               return self.conv._conv_forward(
                   x, 
                   self.conv.weight + 
                   	(self.lora_B @ self.lora_A).view(
                           self.conv.weight.shape) * self.scaling,
                   self.conv.bias
               )
           return self.conv(x)
   
   class Conv2d(ConvLoRA):
       def __init__(self, *args, **kwargs):
           super(Conv2d, self).__init__(nn.Conv2d, *args, **kwargs)
   ```

4. LoRA版本的Transform：

   ![image-20250323184202390](%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250323184202390.png)

   从上图我们可以看到：将所有微调参数都放到attention的某一个参数矩阵的效果并不好，将可微调参数平均分配到Wq和Wk的效果最好

   ```python
   ## using lora in transform. for example q,v 
   # ===== Before =====
   # qkv_proj = nn.Linear(d_model, 3*d_model)
   # ===== After =====
   # Break it up (remember to modify the pretrained checkpoint accordingly)
   q_proj = lora.Linear(d_model, d_model, r=8)
   k_proj = nn.Linear(d_model, d_model)
   v_proj = lora.Linear(d_model, d_model, r=8)
   # Alternatively, use lora.MergedLinear (recommended)
   qkv_proj = lora.MergedLinear(d_model, 3*d_model, r=8, enable_lora=[True, False, True])
   
   ## 具体实现详见代码：https://github.com/microsoft/LoRA/blob/main/loralib/layers.py
   ```

### 5. 内存优化与计算效率分析

|                    | 原始权重                       | **LoRA权重**                         | 参数比例                                                     |
| ------------------ | ------------------------------ | ------------------------------------ | ------------------------------------------------------------ |
| 参数存储           | $d \times k$ 个参数            | $r \times (d + k)$ 个参数            | $\frac{r \times (d + k)}{d \times k} \approx r \times \left(\frac{1}{k} + \frac{1}{d}\right)$ |
| 优化器状态（Adam） | $2 \times d \times k$ 个浮点数 | $2 \times r \times (d + k)$ 个浮点数 | 内存减少：约99.6%                                            |
| 梯度计算与反向传播 | $d \times k$ 个梯度            | $r \times (d + k)$ 个梯度            |                                                              |

### 6. 训练/超参细节

1. **学习率调整**：LoRA通常需要比全参数微调更高的学习率，推荐范围：[5e-4, 1e-3]，是全参数微调学习率的5-10倍

2. **权重衰减处理**：对LoRA权重应用较小的权重衰减，典型值：0.01-0.1之间

3. **秩参数(r)选择**：

   + 原始论文表明，在大多数NLP任务上，r=8足够有效（简单任务，r=4~8）
   + 对于更复杂任务，r=16或r=32可能更合适（生成任务：r=16~32）

4.  **缩放因子α更新**：

   + 常见设置策略：
     + 设置α=r：使缩放因子α/r=1，保持原始更新幅度
     + 设置α=2r：略微增强LoRA更新的影响
     + α的范围通常在[1, 32]之间

   + 与学习率的关系：

     + α与学习率有相乘效应

     + 增大α等效于增加LoRA参数的有效学习率

------------

## ControlNet

### 1. 基本组件

ControlNet首先将原来的模型锁定，称之为locked copy，其中“锁定”副本中的权重保持不变，保留了Stable Diffusion模型原本的能力；与此同时，使用额外数据对“可训练”副本（称之为trainable copy）进行微调，学习我们想要添加的条件，并同时在输入和输出的部分，加上两层卷积。但是不同的是，这两个卷积是**初始化为零的 1×1 卷积层**。

<img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250329203152255.png" alt="image-20250329203152255" style="zoom: 80%;" />

公式表述为：

<img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250329224118836.png" alt="image-20250329224118836" style="zoom: 67%;" />

### 2. 基本架构

<img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250329203343429.png" alt="image-20250329203343429" style="zoom: 67%;" />

1. **将图像空间Condition转化为隐空间Condition**：训练过程中添加了四层卷积层，这些卷积层的卷积核为4×4，步长为2，通道分别为16，32，64，128，初始化为高斯权重，并与整个ControlNet模型进行联合训练。

2. **当prompt对于SD模型不可见时，编码器倾向于从条件输入中学习更多语义，以替代prompt**：研究者随机将 50% 的文本 prompts \( $c_*t$ \) 替换为空字符串，这有利于 ControlNet 从输入条件（例如 Canny 边缘图或人类涂鸦等）中识别语义内容的能力。

3. **Classifier-free guidance resolution weighting推理** ：
   $$
   \epsilon_{prd} = \epsilon_{uc} + \beta_{cfg} (\epsilon_c - \epsilon_{uc})
   $$
   在上面的式子中，$\epsilon_{prd}$ 代表模型最终的输出，$\epsilon_{uc}$ 代表模型 unconditional output（可以理解为普通 stable diffusion 的结果），$\epsilon_c$ 表示模型 conditional output，$\beta_{cfg}$ 是用户可调整的超参数。

   但是这样还存在一个问题：假设存在一个 Canny 处理后的图像，输入进网络作为 condition 之后，可能会完全去除 CFG 的指导，但是如果只用 condition，可能又会让指导过于强烈。

   解决方案是将条件图像先给到 $\epsilon_c$ 当中，然后根据每个块的分辨率，将每个连接在 Stable Diffusion 和 controlNet 乘以一个系数 $w_i=64/h_i$，其中 $h_i$ 是第 i 个特征块的大小，通过逐渐降低 CFG 指导的强度，可以得到较好的效果。

### 3. 关键点剖析

+ **如果zero convolution模块的初始权重为零，那么梯度也为零，ControlNet模型将不会学到任何东西。那么为什么“zero convolution模块”有效呢？**

  假设ControlNet的权重为：$y=wx+b$ ，可以得到对应的梯度求导：
  $$
  \partial y/ \partial w=x, \partial y/\partial x=w, \partial y/\partial b=1
  $$
  

  如果此时 $w=0$ 并且$x\neq0$, 可以得到:
  $$
  \partial y / \partial w\neq0 , \partial y /\partial x =0 , \partial y / \partial b \neq0
  $$
  这就意味着只要 $x\neq0$ ，一次梯度下降迭代将使w变成非零值。然后就得到: $\partial y / \partial x \neq0$ 。这样就能让zero convolution模块逐渐成为具有非零权重的公共卷积层，并不断优化参数权重。

+ **ControlNet 为什么要在trainable copy前后两部分都加入zero convolution呢？**

  （来自QwQ-32B回答）两层零卷积，分别位于可训练副本网络的输入端和输出端。

  + 输入端的特征转换 ：在控制条件输入 ControlNet 时，通过 zero convolution 将输入特征映射到与主干网络特征空间兼容的维度。由于初始权重为零，这一层在训练初期几乎不改变输入，但随着训练进行，权重会逐渐调整以更好地对齐控制条件与主干网络的特征。 

  + 输出端的渐进融合 ：在 ControlNet 的输出阶段，通过另一层 zero convolution 将控制分支的特征与主干网络的特征融合。这层的作用是逐步调整控制信号的强度，确保叠加后的特征既包含控制信息，又不会破坏主干网络原有的特征结构。

+ **ControlNet在 DiT领域如何工作呢？**

  列举了一些文章/仓库：

  1. [RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers](https://arxiv.org/pdf/2502.14377)

     <img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250329222704647.png" alt="image-20250329222704647" style="zoom: 50%;" />

  2. [Hunyuan-DiT, 腾讯混元](https://github.com/Tencent/HunyuanDiT/tree/main/controlnet)

+ **训练显存和时间变化**：

  在单个Nvidia A100 PCIE 40G的环境下，实际应用到Stable Diffusion模型的训练中，ControlNet仅使得每次迭代所需的GPU显存增加大约23%，时间增加34%左右。

------------

## Min-SNR Weighting Strategy

### 1. 从ϵ-prediction到v-prediction

1. 区别：

   + ϵ-prediction：噪声预测型，直接预测在每个时间步添加的噪声
   + v-prediciton：速度预测型，重点在于预测数据如何在时间序列中演变；

2. 重新审视加躁模型：

   + **从加噪的过程看**：干净图像和高斯噪声的权重平方是 $\bar{\alpha_t}+(1−\bar{\alpha_t})= 1$，那么就可以用**单位向量画圆**来理解干净图像和高斯噪声的关系：y轴就是干净图像，x轴是高斯噪声，z就是xt，即：从干净图像开始，以一定的步长，混合一定比例的高斯噪声，沿着圆的弧线变化，直到最后完全变成高斯噪声。

     <img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250413231219567.png" alt="image-20250413231219567" style="zoom: 25%;" />

     此时损失函数为：
     $$
     L_\theta = \left\| \epsilon - \epsilon_{\theta} \left( \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t \right) \right\|^2
     $$
     信噪比为：
     $$
     \text{SNR}_{ddpm} = \frac{\| x_{signal} \|^2}{\| x_{noise} \|^2} = \frac{\overline{\alpha_t}}{1-\overline{\alpha_t}}
     $$

   + **扩展到速度预测模型**：令沿着弧线的这个切向速度（可以分解为增加的噪声分量和减少的信号分量）为向量 $v_t$ ，有：
     $$
     \left\{
             \begin{array}{l}
                 v_t = \frac{dx_t}{d\phi} = \frac{d\cos\phi}{d\phi}x_0 + \frac{d\sin\phi}{d\phi}\epsilon = \sqrt{\bar{\alpha}_t}\epsilon - \sqrt{1-\bar{\alpha}_t}x_0 \\
                 x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon
             \end{array}
         \right.
     $$
     可以通过二元一次方程求解得到：
     $$
     x_0 = \sqrt{\bar{\alpha}_t}x_t - \sqrt{1-\bar{\alpha}_t}\ v_t
     $$
     此时损失函数为：
     $$
     \begin{aligned}
     L_\theta &= \| v - v_\theta(x_t, t) \|^2 \\
     &=  \frac{1}{1-\bar{\alpha_t}}\left\| (x_0 - x_\theta(x_t, t)) \right\|^2 \\
     &=  (\text{SNR}_{ddpm} + 1)\left\|(x_0 - x_\theta(x_t, t)) \right\|^2
     \end{aligned}
     $$

   + **结论**：v-prediction 学习的是噪声变化的速度，其 loss weight 刚好比ϵ-prediction（DDPM）的SNR多一个1，缓解了在去噪前期的信号弱问题，训练相对稳定一些。

3. 参考论文：Progressive distillation for fast sampling of diffusion models

### 2. 关键点分析

通过实验证明，扩散模型收敛速度慢的原因为**训练过程中不同时间步的优化方向相互冲突，不同噪声水平下的最优权重梯度是相互冲突的。**

<img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250413233838523.png" alt="image-20250413233838523" style="zoom: 67%;" />

针对这一问题，提出了**Min - SNR - γ**损失加权策略。该策略将每个时间步的去噪过程视为单独的任务，从而扩散训练可以被认为是一个多任务学习问题，并通过难易程度为其分配损失权重。

**Min - SNR - γ**损失加权策略：与耗时的帕累托优化相比，是次优策略，但是更加简洁且不耗时。论文设置一个超参数 γ（默认是5），对于比较简单的case（t比较小），权重最大为5，迫使网络不要关注简单case；而对于比较难的case，权重保持不变，以此让网络关注难样本。
$$
L_{\theta} = \left\| \min(\text{SNR}_{ddpm}+1, \gamma) (x_0 - x_\theta(x_t, t)) \right\|^2
$$


---------------

##  SD3 和 FLUX.1

+ 论文：Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
+ 参考链接：
  1. https://arxiv.org/pdf/2403.03206：SD3论文
  1. https://arxiv.org/pdf/2412.06264：Flow Matching 论文
  1. https://arxiv.org/pdf/2209.03003：Rectified Flow 论文
  1. https://zhouyifan.net/2024/07/14/20240703-SD3/：SD3论文介绍
  1. https://zhuanlan.zhihu.com/p/11722282090：Flow Matching梳理与揭秘
  1. https://zhuanlan.zhihu.com/p/15125790338：Rectified 讲述可视化过程
  1. https://zhuanlan.zhihu.com/p/11686643707：Rectified 深入理论细节

### 1. 核心贡献

- **参数扩增**：在大型文生图模型上使用了整流模型，使用DiT来更好地融合文本信息
- **技术创新**：
  - 非均匀采样用于调整时间步 t 的采样分布，从而影响信噪比（SNR）；
  - 提升自编码器通道数：把隐空间图像的通道数从 4 改为了 16；
  - 使用二维位置编码来实现任意分辨率的图像生成；
  - 提升 VAE 隐空间通道数；
  - 对注意力 QK 做归一化以确保高分辨率下训练稳定；

### 2. Flow Matching 算法

流匹配(Flow Matching) 是一种定义图像生成目标的方法，可以兼容扩散模型的训练目标，代表性的工作是整流 (rectified flow)；

1. 目标：找到一条“平滑、正确”的路径，把起点的随机噪声逐步变成目标分布（$x_1$表示真实数据，$x_0$表示随机噪声）；

2. 优势：流匹配模型和扩散模型的另一个区别是，流匹配模型天然支持 image2image 任务。从纯噪声中生成图像只是流匹配模型的一个特例；

3. 过程：将生成过程拆分成多个时间步长，每一步都模拟从一个分布“流动”到下一个分布；

4. 数学：令模型预一个向量场 $v(x(t),t)$ ，有：

   + $t$ 时刻的概率密度路径：$p_t(x) = \int p_t(x|x_1) q(x_1) dx_1$，

     + 表示在 t 时刻所有产生 $x$ 的 $x_1$ 做累计（积分），从而得出 $t$ 时刻下 $x$ 的概率密度；
     + 其中，$q(x_1)$ 服从真实图像分布；

   + $t$ 时刻的条件概率路径：服从高斯分布，$p_t(x|x_1) = \mathcal{N}(x|\mu_t(x_1), \sigma^2_t(x_1)\mathcal{I})$，由采样定理可得 $x = \sigma_t(x_1)z + \mu_t(x_1)$；

     + 时间 $t=0$ 时， $p_0(x|x_1) = p(x)$，此时与样本数据 $x_1$ 无关，是一个标准噪声分布；
     + $t=1$ 时要大致符合数据分布，即 $p_1(x) \approx q(x)$。

   + $t$ 时刻的平均速度向量：$u_t(x) = \int u_t(x|x_1) \frac{p_t(x|x_1) q(x_1)}{p_t(x)} dx_1$；

     + 其中 $u_t(x|x_1)$为在 $t$ 时刻速度方向， $p_t(x|x_1)q(x_1)$ 代表单个链路，$\frac{p_t(x|x_1)q(x_1)}{p_t(x)}$ 代表单个链路与总体分布的比重，所以上述公式代表的是所有链路速度方向的平均；
     + 定理：给定条件概率路径 $p(x|x_1)$ 以生成该路径的条件向量场 $u(x|x_1)$，对于任意数据分布 $q(x_1)$，边缘向量场 $u_t$ 和 $p_t$ 满足连续性方程，即 $u_t$ 能够生成 $p_t$。

   + $t$ 时刻单个链路的速度方向：
     $$
     \begin{aligned}
     u_t(x|x_1) &= \frac{dx_t}{dt}
     = \sigma'_t(x_1)z + \mu'_t(x_1)
     = \sigma'_t(x_1)\frac{x - \mu_t(x_1)}{\sigma_t(x_1)} + \mu'_t(x_1) \\
     &= \frac{\sigma'_t(x_1)}{\sigma_t(x_1)}(x - \mu_t(x_1)) + \mu'_t(x_1)
     \end{aligned}
     $$

   + 链路设计：

     + 一般情况：$\mu_t(x_1) = \alpha x_1$，$\sigma_t(x_1) = \beta_t$

     + Diffusion Conditional VFs （扩散条件传输）：均值和标准差分别为 $\mu_t(x_1) = \alpha_{1-t}x_1$，$\sigma_t(x_1) = \sqrt{1-\alpha_{1-t}^2}$。
     
     + Optimal Transport Conditional VFs（条件最优传输）：将均值和标准差定义为简单的线性变换，此时有：
       $$
       \begin{aligned}
       \mu_t(x_1) &= tx_1\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mu'_t(x_1) = x_1 \\
       \sigma_t(x_1) &= 1 - (1 - \sigma_{min})t \ \ \ \ \ \  \sigma'_t(x_1) = -(1 - \sigma_{min})
       \end{aligned}
       $$
       单链路速度方向为：$u_t(x|x_1) = \frac{x_1 - (1 - \sigma_{min})x}{1 - (1 - \sigma_{min})t}$

5. 训练与推理：

   + 训练阶段：已知 $x_1$，获得 $t$ 时刻的状态 $x = \sigma_t(x_1)z + \mu_t(x_1)$，由此便可以计算出单个链路的速度向量 $u_t(x|x_1) = \frac{x_1 - (1 - \sigma_{min})x}{1 - (1 - \sigma_{min})t}$，以此为目标来优化模型 $v_t(x)$。与 loss 优化一致，虽然每一次优化是单个样本的结果，最终会让模型超各个速度角度都均衡的方向优化，最终近似平均的加权速度向量 $u_t(x)$。

   + 推理阶段，从 $0 \sim 1$，采样步长为 $\Delta t$，由此可得：

     $$
     \begin{aligned}
     x_{\Delta t} &= x_0 + v_0(x)\Delta t \\
     x_{2\Delta t} &= x_{\Delta t} + v_{\Delta t}(x)\Delta t \\
     x_{3\Delta t} &= x_{2\Delta t} + v_{2\Delta t}(x)\Delta t \\
     &\vdots \\
     x_1 &= x_{1-\Delta t} + v_{1-\Delta t}(x)\Delta t
     \end{aligned}
     $$

     由此，经过 $\frac{1}{\Delta t}$ 个时间步就可以得到 $x_1$。

   + 损失函数：由于$u_t(x)$不易计算，因此可以转换为$u_t(x | x_1)$，且两者只相差常数项；
     $$
     \begin{aligned}
     L_{FM} &= \mathbb{E}_{t, x(t)} \left[ \| v(x(t), t) - u_t(x) \|^2 \right] \\
     L_{CFM} &= \mathbb{E}_{t, x(t)} \left[ \| v(x(t), t) - u_t(x | x_1) \|^2 \right]
     \end{aligned}
     $$

     + 证明为什么等价：
       $$
       \begin{aligned}
       & \text{FM: }  \|v_t(x) - u_t(x)\|^2 = \|v_t(x)\|^2 - 2\langle v_t(x), u_t(x) \rangle + \|u_t(x)\|^2 \\
       
       & \text{CFM: }  \|v_t(x) - u_t(x|x_1)\|^2 = \|v_t(x)\|^2 - 2\langle v_t(x), u_t(x|x_1) \rangle + \|u_t(x|x_1)\|^2 \\
       
       \mathbb{E}_{p_t(x)} \|v_t(x)\|^2 &= \int \|v_t(x)\|^2 p_t(x) dx \\
       
       \quad &= \int \|v_t(x)\|^2 \underbrace{\int p_t(x|x_1)q(x_1)dx_1}_{p_t(x)} dx \\ 
       
       \quad &= \mathbb{E}_{q(x_1), p_t(x|x_1)} \|v_t(x)\|^2 \\
       
       \mathbb{E}_{p_t(x)} \langle v_t(x), u_t(x) \rangle &= \int \left\langle v_t(x), \underbrace{\int u_t(x|x_1)p_t(x|x_1)q(x_1)dx_1}_{u_t(x)} \right\rangle p_t(x) dx \\
       
       \quad &= \int \left\langle v_t(x), \int u_t(x|x_1)p_t(x|x_1)q(x_1)dx_1 \right\rangle dx \\
       
       \quad &= \int \langle v_t(x), u_t(x|x_1) \rangle p_t(x|x_1)q(x_1)dx_1dx \\
       
       \quad &= \mathbb{E}_{q(x_1), p_t(x|x_1)} \langle v_t(x), u_t(x|x_1) \rangle
       \end{aligned}
       $$

### 3. Rectified Flow 算法

+ **算法优势**：只需一步计算就直接产生高质量的结果，而不需要调用计算量大的数值求解器来迭代式地模拟整个扩散过程。

+ **区别**：从插值的角度，DDPM建模的是相对比较复杂的曲线运动，而RF建模的就是非常简单的直线运动。

+ **Rectified 操作**：

  + 如果存在函数 $X$ 使得对任意 $x_0 \sim$ 正态，$x_1 \sim$ 图片分布，$x_t = X_t(x_0, x_1)$ 关于时间 $t$ 可微且 $X_{t=0}(x_0, x_1) = x_0, \quad X_{t=1}(x_0, x_1) = x_1$；
    + 随机插值 flow 中：$X_t = t X_1 + (1 - t)X_0 + \sigma(t)\cdot z$
    + Rectified flow 中：$X_t = t X_1 + (1 - t)X_0$， 此时 $\frac{dX_t(x_0, x_1)}{dt} = x_1 - x_0$
  + $v(x_t, t)$ 即为所求 ODE 的向量场。$X_t$ 沿时间 $t$ 把 $x_0$ 与 $x_1$ 连成了一条路径，所以它就是穿过点 $x_t$ 的所有路径上的导数的均值。转化为 loss 为：

  $$
  L_{RF}(\theta) = \mathbb{E}_{t, x_0, x_1} \left\| \frac{dX_t(x_0, x_1)}{dt} - v_t(X_t, t) \right\|_2^2
  			 = \mathbb{E}_{t, x_0, x_1} \left\| (X_1 - X_0) - v_t(X_t, t) \ \right\|_2^2
  $$

  ​	优化为：
  $$
  \min_v \int_0^1 \mathbb{E}_{X_0 \sim \pi_0, X_1 \sim \pi} \left[ \left| (X_1 - X_0) - v(X_t, t) \right|^2 \right] dt, \quad \text{with} \quad X_t = tX_1 + (1-t)X_0.
  $$

+ **Reflow 操作**：
  
  + 问题在一开始时，$X_1$和$X_0$ 需要随机进行匹配，但是要在直线轨迹的交叉点做路径重组，避免非因果关系；
  
  + 具体操作：
    1. 从 $\pi_0$ 里采样出一批 $X_0$。然后，从 $X_0$ 出发，模拟上面学出的 flow（1-Rectified Flow），得到 $X_1 = \text{Flow}_1(X_0)$。并用这样得到的 $(X_0, X_1)$ 对来学一个新的 "2-Rectified Flow"；2-Rectified Flow 和 1-Rectified Flow 在训练过程中唯一的区别就是数据配对不同：在 1-Rectified Flow 中，$X_0$ 与 $X_1$ 是随机或者任意配对的；在 2-Rectified Flow 中，$X_0$ 与 $X_1$ 是通过 1-Rectified Flow 配对的。
    
    2. 因为从 1-Rectified Flow 里出来的 $(X_0, X_1)$ 已经有很好的配对，他们的直线插值交叉数减少，所以 2-Rectified Flow 的轨迹也就（比起 1-Rectified Flow）变得很直了；但注意这里Rectified flow的采样轨迹，只有在执行了多次（一次）的rectification的操作之后，才会慢慢的变为直线；
    
    3. 最终表述为：
       $$
       \min_v \int_0^1 \mathbb{E}_{X_0 \sim \pi_0, X_1 = \text{Flow}_1{(X_0)}} \left[ \left| (X_1 - X_0) - v(X_t, t) \right|^2 \right] dt, \quad \text{with} \quad X_t = tX_1 + (1-t)X_0.
       $$
       只有 X1 的采样方式变了，变成根据已经训练好的模型对 X0 进行分布转换的结果。
    
    4. 2-Rectified是贴近由训练好的1-Rectified产生的配对点的插值方程，所以从理论上来说，**2-Rectified的生成效果是要小于等于1-Rectified的**，只能加速。
    
       所以整个Rectified Flow的流程是，第一次训练拟合与插值边缘分布一致的ODE，得到生成模型，无法一步采样；第二次训练拟合插值的速度，贴近插值方程（直线），实现一步采样。
    
  + 可视化代码参考：https://colab.research.google.com/drive/1CyUP5xbA3pjH55HDWOA8vRgk2EEyEl_P?usp=sharing
  
+ **扩展 Recfified Diffusion**：
  
  + Reflow操作的本质是使用配对的噪声样本对重训练。本质上，通过引入预训练模型来收集噪声-样本对，并在标准的扩散训练中用这些预先收集的配对替换随机采样的噪声和真实样本，我们就得到了Rectified Diffusion的训练算法。
  
  + 利用配对的数据再次2-Rectified也可以加速DDIM，甚至是加速任意之前提到的扩散模型，虽然跟Rectified Flow用直线一步加速不太一样，但是本质都是贴近插值方程曲线
  
    <img src="%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250504194907944.png" alt="image-20250504194907944" style="zoom: 43%;" />
  
### 4.  LogNorm 时间步采样器

作者主观认为，当 timestep 位于 中间值时，预测的难度增加。这是因为在 t=T  时，模型的最佳预测是基于高斯分布的均值，而在 t=0  时，最佳预测则是基于最终干净图像/视频分布的均值。而在中间状态，模型需要同时考虑两个分布的信息，因此从统计和概率上来看，预测更加复杂。

**为了采样更多的中间步长，SD3采用了对数正态分布**，任意随机变量的对数服从正态分布,则这个随机变量服从的分布称为对数正态分布。
$$
f(x)=\begin{cases}
\displaystyle \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln x-\mu)^2}{2\sigma^2}} & x > 0 \\
0 & \text{其他情况}
\end{cases}
$$

- SD3默认：$\mu=0，\sigma=1$；

### 5. 网络架构

1. 基本框架图：

   ![image-20250504224303124](%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250504224303124.png)

   1. Text Encoder：
      + 采用CLIP+T5-XXL（两个CLIP模型和T5共3个）以及一个改进的自编码模型来编码图像令牌；

   2. QK归一化：
      + 通过将Q和K除以其范数来保持注意力权重的稳定性，避免在训练后期出现注意力熵崩溃的问题。

   3. 不同宽高比的位置编码：

      + 计算最大尺寸：根据目标分辨率计算出可能遇到的最大宽度和高度（例如，$W_{max}$ 和 $H_{max}$）。

      + 创建位置网格：基于这些最大尺寸，创建一个垂直和水平的位置网格。对于垂直位置，可以使用以下公式：
        $$
        \text{vertical positions} = \left( \left(p - \frac{h_{max} - s}{2}\right) \cdot \frac{S}{256} \right)^{h_{max}-1}_{p=0}
        $$
        其中 $s = \frac{S}{16}$ 是缩放因子。
      
       + 中心裁剪：从生成的位置网格中中心裁剪，以适应实际的图像尺寸。
           
           通过这种方式，位置编码可以适应不同的长宽比，确保模型在处理可变尺寸的图像时能够准确地捕捉位置信息。这种方法在处理文本到图像生成任务时尤为重要，因为输入图像的长宽比可能会变化很大。
   
