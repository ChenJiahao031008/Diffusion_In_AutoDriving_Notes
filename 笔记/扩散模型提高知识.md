# 扩散模型提高知识

------------------

## Low Rank Adaptation (LoRA)

https://arxiv.org/pdf/2106.09685

###  1. LoRA的出现的原因

+ 使用LORA，训练参数仅为整体参数的万分之一、GPU显存使用量减少2/3且不会引入额外的推理耗时

### 2. 低秩假设：LoRA的理论基础

**预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果**。具体而言：

1. **权重更新的低秩特性**：矩阵秩表示其线性无关的行或列的数量。研究发现，微调过程中权重的变化ΔW可以用远低于其原始维度的向量组合表示。
2. **内在维度**：在预训练后，越大的模型有越小的内在维度，这也解释了为何大模型都拥有很好的few-shot能力。

### 3. LoRA微调

![image-20250323165613779](%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250323165613779.png)

公式： $h = W_0x+ \Delta Wx = W_0x + \frac{\alpha}{r}\cdot B Ax$。

+ 其中 $W_0 \in \R ^{(d\times r)}$ 是预训练的权重矩阵，$\Delta W = BA$ 就是我们添加的额外矩阵，$B \in \R ^ {(d \times r)}, A \in \R^{r\times k}$，$r \ll \min(d,k)$ 表示矩阵的秩，为超参。

+ 初始化时，矩阵 A 随机高斯初始化$N(0, \sigma^2)$初始化，其中$\sigma = 1/\sqrt{r}$，矩阵 B 初始化为0：在初始阶段这两个矩阵相乘为0，可以保证在初始阶段时，只有左边的主干生效。然后 BA 还会乘以一个缩放因子 $\frac{\alpha}{r}$ ，α也是超参。
+ 训练的时候，预训练的权重矩阵全部都是冻结的，仅训练A和B中的参数。

### 4. Lora 实现细节

1. **参数合并机制** ：LoRA不会增加额外的推理时间成本

   + **微调后合并权重** ：在LoRA训练完成后，通常会将低秩增量矩阵（ΔW）直接合并到原始模型的权重矩阵（W）中，形成新的权重矩阵 W′=W+ΔW。**推理时直接使用合并后的权重**，无需额外计算步骤，因此计算复杂度与原始模型完全一致。

2. LoRA版本的全连接层：

   ```python
   # 在调用凯明初始化的时候注释里写的高斯分布，调用的却是均匀分布，而且参数a的值设置的是根号5，但a表示的是leaky relu的负斜率系数，一般是0.01这样的小值，不可能超过1
   def reset_parameters(self):
       nn.Linear.reset_parameters(self)
       if hasattr(self, 'lora_A'):
           # initialize A the same way as the default for nn.Linear and B to zero
           nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
           nn.init.zeros_(self.lora_B)
           
   def forward(self, x: torch.Tensor):
       def T(w):
           return w.transpose(0, 1) if self.fan_in_fan_out else w
       if self.r > 0 and not self.merged:
           result = F.linear(x, T(self.weight), bias=self.bias)            
           result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling
           return result
       else:
           return F.linear(x, T(self.weight), bias=self.bias)
   ```

3. LoRA版本的卷积层：

   将增量 $\Delta W \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}} \times K_h \times K_w}$ 分解为两个低秩矩阵：
   - $$ A \in \mathbb{R}^{C_{\text{out}} \times r \times 1 \times 1} $$（逐点卷积，调整通道维度）
   - $$ B \in \mathbb{R}^{r \times C_{\text{in}} \times K_h \times K_w} $$（低秩卷积核，捕捉空间特征）  

   ```python
    ## CASE 1: https://blog.csdn.net/weixin_54338498/article/details/136811439
       if isinstance(module, nn.Conv2d):
           assert len(self._up_weight.shape) == len(self._down_weight.shape) == 4
   
           r = self._r
           in_dim = module.in_channels
           out_dim = module.out_channels
           kernel = module.kernel_size
           stride = module.stride
           padding = module.padding
   
           self._lora_down = nn.Conv2d(in_dim, r, kernel, stride, padding, bias=False)
           self._lora_up = nn.Conv2d(r, out_dim, (1, 1), (1, 1), bias=False)
   
       self._lora_down.weight = nn.Parameter(self._down_weight)
       self._lora_up.weight = nn.Parameter(self._up_weight)
       
    ## CASE 2: https://zhuanlan.zhihu.com/p/658007966
    ##         https://github.com/microsoft/LoRA/blob/main/loralib/layers.py
   	def __init__(....):
           self.lora_A = nn.Parameter(
               self.conv.weight.new_zeros((r * kernel_size, in_channels * kernel_size))
           )
           self.lora_B = nn.Parameter(
             self.conv.weight.new_zeros((
                 out_channels//self.conv.groups * kernel_size, r * kernel_size))
           )
           self.scaling = self.lora_alpha / self.r
       
       def train(self, mode=True):
   		...
           self.conv.weight.data += 
               (self.lora_B @ self.lora_A).view(self.conv.weight.shape) * self.scaling
       	self.merged = True
   
       def forward(self, x):
           if self.r > 0 and not self.merged:
               return self.conv._conv_forward(
                   x, 
                   self.conv.weight + 
                   	(self.lora_B @ self.lora_A).view(
                           self.conv.weight.shape) * self.scaling,
                   self.conv.bias
               )
           return self.conv(x)
   
   class Conv2d(ConvLoRA):
       def __init__(self, *args, **kwargs):
           super(Conv2d, self).__init__(nn.Conv2d, *args, **kwargs)
   ```

4. LoRA版本的Transform：

   ![image-20250323184202390](%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8F%90%E9%AB%98%E7%9F%A5%E8%AF%86.assets/image-20250323184202390.png)

   从上图我们可以看到：将所有微调参数都放到attention的某一个参数矩阵的效果并不好，将可微调参数平均分配到Wq和Wk的效果最好

   ```python
   ## using lora in transform. for example q,v 
   # ===== Before =====
   # qkv_proj = nn.Linear(d_model, 3*d_model)
   # ===== After =====
   # Break it up (remember to modify the pretrained checkpoint accordingly)
   q_proj = lora.Linear(d_model, d_model, r=8)
   k_proj = nn.Linear(d_model, d_model)
   v_proj = lora.Linear(d_model, d_model, r=8)
   # Alternatively, use lora.MergedLinear (recommended)
   qkv_proj = lora.MergedLinear(d_model, 3*d_model, r=8, enable_lora=[True, False, True])
   
   ## 具体实现详见代码：https://github.com/microsoft/LoRA/blob/main/loralib/layers.py
   ```

### 5. 内存优化与计算效率分析

|                    | 原始权重                       | **LoRA权重**                         | 参数比例                                                     |
| ------------------ | ------------------------------ | ------------------------------------ | ------------------------------------------------------------ |
| 参数存储           | $d \times k$ 个参数            | $r \times (d + k)$ 个参数            | $\frac{r \times (d + k)}{d \times k} \approx r \times \left(\frac{1}{k} + \frac{1}{d}\right)$ |
| 优化器状态（Adam） | $2 \times d \times k$ 个浮点数 | $2 \times r \times (d + k)$ 个浮点数 | 内存减少：约99.6%                                            |
| 梯度计算与反向传播 | $d \times k$ 个梯度            | $r \times (d + k)$ 个梯度            |                                                              |

### 6. 训练/超参细节

1. **学习率调整**：LoRA通常需要比全参数微调更高的学习率，推荐范围：[5e-4, 1e-3]，是全参数微调学习率的5-10倍

2. **权重衰减处理**：对LoRA权重应用较小的权重衰减，典型值：0.01-0.1之间

3. **秩参数(r)选择**：

   + 原始论文表明，在大多数NLP任务上，r=8足够有效（简单任务，r=4~8）
   + 对于更复杂任务，r=16或r=32可能更合适（生成任务：r=16~32）

4.  **缩放因子α更新**：

   + 常见设置策略：
     + 设置α=r：使缩放因子α/r=1，保持原始更新幅度
     + 设置α=2r：略微增强LoRA更新的影响
     + α的范围通常在[1, 32]之间

   + 与学习率的关系：

     + α与学习率有相乘效应

     + 增大α等效于增加LoRA参数的有效学习率

------------

## ControlNet



------------

## Flow Matching

